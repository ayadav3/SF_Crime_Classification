{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from copy import deepcopy\n",
    "# %matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDF=pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "abc\n",
      "abc\n",
      "abc\n"
     ]
    }
   ],
   "source": [
    "xy_scaler=preprocessing.StandardScaler()\n",
    "print \"abc\"\n",
    "xy_scaler.fit(trainDF[[\"X\",\"Y\"]])\n",
    "print \"abc\"\n",
    "trainDF[[\"X\",\"Y\"]]=xy_scaler.transform(trainDF[[\"X\",\"Y\"]])\n",
    "print \"abc\"\n",
    "trainDF=trainDF[abs(trainDF[\"Y\"])<100]\n",
    "print \"abc\"\n",
    "trainDF.index=range(len(trainDF))\n",
    "# plt.plot(trainDF[\"X\"],trainDF[\"Y\"],'.')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NX=100\n",
    "NY=100\n",
    "groups = trainDF.groupby('Category')\n",
    "ii=1\n",
    "plt.figure(figsize=(20, 20))\n",
    "for name, group in groups:\n",
    "    plt.subplot(8,5,ii)\n",
    "    histo, xedges, yedges = np.histogram2d(np.array(group.X),np.array(group.Y), bins=(NX,NY))\n",
    "    myextent  =[xedges[0],xedges[-1],yedges[0],yedges[-1]]\n",
    "    plt.imshow(histo.T,origin='low',extent=myextent,interpolation='nearest',aspect='auto',norm=LogNorm())\n",
    "    plt.title(name)\n",
    "#     plt.figure(ii)\n",
    "#     plt.plot(group.X,group.Y,'.')\n",
    "    ii+=1\n",
    "del groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    return time,day,month,year\n",
    "\n",
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_data(df,logodds,logoddsPA):\n",
    "    feature_list=df.columns.tolist()\n",
    "    if \"Descript\" in feature_list:\n",
    "        feature_list.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature_list:\n",
    "        feature_list.remove(\"Resolution\")\n",
    "    if \"Category\" in feature_list:\n",
    "        feature_list.remove(\"Category\")\n",
    "    if \"Id\" in feature_list:\n",
    "        feature_list.remove(\"Id\")\n",
    "    cleanData=df[feature_list]\n",
    "    cleanData.index=range(len(df))\n",
    "    print \"Creating address features\"\n",
    "    address_features=cleanData[\"Address\"].apply(lambda x: logodds[x])\n",
    "    address_features.columns=[\"logodds\"+str(x) for x in range(len(address_features.columns))]\n",
    "    print \"Parsing dates\"\n",
    "    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\n",
    "#     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "#     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\n",
    "    print \"Creating one-hot variables\"\n",
    "    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\n",
    "    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\n",
    "    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n",
    "    cleanData[\"logoddsPA\"]=cleanData[\"Address\"].apply(lambda x: logoddsPA[x])\n",
    "    print \"droping processed columns\"\n",
    "    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\n",
    "    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Address\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Dates\",axis=1)\n",
    "    feature_list=cleanData.columns.tolist()\n",
    "    print \"joining one-hot features\"\n",
    "    features = cleanData[feature_list].join(dummy_ranks_PD.ix[:,:]).join(dummy_ranks_DAY.ix[:,:]).join(address_features.ix[:,:])\n",
    "    print \"creating new features\"\n",
    "    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(take_last=True)).apply(int)\n",
    "    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in df.columns:\n",
    "        print df[\"Category\"]\n",
    "        labels = df[\"Category\"].astype('category')\n",
    "        #labels = df[\"Category\"].unique()\n",
    "#         label_names=labels.unique()\n",
    "#         labels=labels.cat.rename_categories(range(len(label_names)))\n",
    "    else:\n",
    "        labels=None\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addresses=sorted(trainDF[\"Address\"].unique())\n",
    "categories=sorted(trainDF[\"Category\"].unique())\n",
    "C_counts=trainDF.groupby([\"Category\"]).size()\n",
    "A_C_counts=trainDF.groupby([\"Address\",\"Category\"]).size()\n",
    "A_counts=trainDF.groupby([\"Address\"]).size()\n",
    "logodds={}\n",
    "logoddsPA={}\n",
    "MIN_CAT_COUNTS=2\n",
    "default_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\n",
    "for addr in addresses:\n",
    "    PA=A_counts[addr]/float(len(trainDF))\n",
    "    logoddsPA[addr]=np.log(PA)-np.log(1.-PA)\n",
    "    logodds[addr]=deepcopy(default_logodds)\n",
    "    for cat in A_C_counts[addr].keys():\n",
    "        if (A_C_counts[addr][cat]>MIN_CAT_COUNTS) and A_C_counts[addr][cat]<A_counts[addr]:\n",
    "            PA=A_C_counts[addr][cat]/float(A_counts[addr])\n",
    "            logodds[addr][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\n",
    "    logodds[addr]=pd.Series(logodds[addr])\n",
    "    logodds[addr].index=range(len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating address features\n",
      "Parsing dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n",
      "creating new features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:35: FutureWarning: the take_last=True keyword is deprecated, use keep='last' instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                       WARRANTS\n",
      "1                 OTHER OFFENSES\n",
      "2                 OTHER OFFENSES\n",
      "3                  LARCENY/THEFT\n",
      "4                  LARCENY/THEFT\n",
      "5                  LARCENY/THEFT\n",
      "6                  VEHICLE THEFT\n",
      "7                  VEHICLE THEFT\n",
      "8                  LARCENY/THEFT\n",
      "9                  LARCENY/THEFT\n",
      "10                 LARCENY/THEFT\n",
      "11                OTHER OFFENSES\n",
      "12                     VANDALISM\n",
      "13                 LARCENY/THEFT\n",
      "14                  NON-CRIMINAL\n",
      "15                  NON-CRIMINAL\n",
      "16                       ROBBERY\n",
      "17                       ASSAULT\n",
      "18                OTHER OFFENSES\n",
      "19                  NON-CRIMINAL\n",
      "20                 LARCENY/THEFT\n",
      "21                       ROBBERY\n",
      "22                      WARRANTS\n",
      "23                  NON-CRIMINAL\n",
      "24                 LARCENY/THEFT\n",
      "25                  NON-CRIMINAL\n",
      "26                 LARCENY/THEFT\n",
      "27                 LARCENY/THEFT\n",
      "28                 LARCENY/THEFT\n",
      "29                OTHER OFFENSES\n",
      "                   ...          \n",
      "877952            OTHER OFFENSES\n",
      "877953            OTHER OFFENSES\n",
      "877954                 VANDALISM\n",
      "877955             VEHICLE THEFT\n",
      "877956             LARCENY/THEFT\n",
      "877957            OTHER OFFENSES\n",
      "877958            OTHER OFFENSES\n",
      "877959                  WARRANTS\n",
      "877960                  WARRANTS\n",
      "877961                   ASSAULT\n",
      "877962            OTHER OFFENSES\n",
      "877963     SEX OFFENSES FORCIBLE\n",
      "877964                   ASSAULT\n",
      "877965            OTHER OFFENSES\n",
      "877966                 VANDALISM\n",
      "877967                  TRESPASS\n",
      "877968                   ASSAULT\n",
      "877969             LARCENY/THEFT\n",
      "877970                 VANDALISM\n",
      "877971                  WARRANTS\n",
      "877972            OTHER OFFENSES\n",
      "877973                   ASSAULT\n",
      "877974            OTHER OFFENSES\n",
      "877975                   ASSAULT\n",
      "877976            OTHER OFFENSES\n",
      "877977                   ROBBERY\n",
      "877978             LARCENY/THEFT\n",
      "877979             LARCENY/THEFT\n",
      "877980                 VANDALISM\n",
      "877981    FORGERY/COUNTERFEITING\n",
      "Name: Category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "features, labels=parse_data(trainDF,logodds,logoddsPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'Y', 'Time', 'Day', 'Month', 'Year', 'IsInterection', 'logoddsPA', 'PD_BAYVIEW', 'PD_CENTRAL', 'PD_INGLESIDE', 'PD_MISSION', 'PD_NORTHERN', 'PD_PARK', 'PD_RICHMOND', 'PD_SOUTHERN', 'PD_TARAVAL', 'PD_TENDERLOIN', 'DAY_Friday', 'DAY_Monday', 'DAY_Saturday', 'DAY_Sunday', 'DAY_Thursday', 'DAY_Tuesday', 'DAY_Wednesday', 'logodds0', 'logodds1', 'logodds2', 'logodds3', 'logodds4', 'logodds5', 'logodds6', 'logodds7', 'logodds8', 'logodds9', 'logodds10', 'logodds11', 'logodds12', 'logodds13', 'logodds14', 'logodds15', 'logodds16', 'logodds17', 'logodds18', 'logodds19', 'logodds20', 'logodds21', 'logodds22', 'logodds23', 'logodds24', 'logodds25', 'logodds26', 'logodds27', 'logodds28', 'logodds29', 'logodds30', 'logodds31', 'logodds32', 'logodds33', 'logodds34', 'logodds35', 'logodds36', 'logodds37', 'logodds38', 'IsDup', 'Awake', 'Summer', 'Fall', 'Winter', 'Spring']\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print features.columns.tolist()\n",
    "print len(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_feature_list=[\"Time\",\"Day\",\"Month\",\"Year\",\"DayOfWeek\"]\n",
    "collist=features.columns.tolist()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(features)\n",
    "features[collist]=scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07714363  0.06267647  0.04861061  0.04300871  0.03340587  0.03085896\n",
      "  0.02834616  0.02731142  0.02509795  0.02393235  0.02272111  0.02029053\n",
      "  0.02000664  0.019452    0.01918369  0.01820193  0.01729548  0.01724542\n",
      "  0.01715838  0.01714426  0.01703779  0.01695665  0.01668107  0.01651773\n",
      "  0.01626539  0.01588035  0.01489293  0.01472615  0.01422875  0.01412131\n",
      "  0.0136947   0.01330571  0.01234474  0.01181851  0.01158515  0.01126484\n",
      "  0.01104143  0.0105683   0.01032344  0.00943963  0.00903894  0.00838257\n",
      "  0.00819149  0.00795652  0.0074206   0.00726109  0.00707183  0.00675531\n",
      "  0.0067189   0.00646287  0.00634686  0.00612685  0.0060737   0.00556134\n",
      "  0.00526841  0.00516993  0.0048638   0.00452517  0.00417899  0.00407036]\n"
     ]
    }
   ],
   "source": [
    "new_PCA=PCA(n_components=60)\n",
    "new_PCA.fit(features)\n",
    "plt.plot(new_PCA.explained_variance_ratio_)\n",
    "plt.yscale('log')\n",
    "plt.title(\"PCA explained ratio of features\")\n",
    "print new_PCA.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x23d5e748>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(new_PCA.explained_variance_ratio_.cumsum())\n",
    "plt.title(\"cumsum of PCA explained ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(labels, train_size=0.5)\n",
    "for train_index, test_index in sss:\n",
    "    features_train,features_test=features.iloc[train_index],features.iloc[test_index]\n",
    "    labels_train,labels_test=labels[train_index],labels[test_index]\n",
    "features_test.index=range(len(features_test))\n",
    "features_train.index=range(len(features_train))\n",
    "labels_train.index=range(len(labels_train))\n",
    "labels_test.index=range(len(labels_test))\n",
    "features.index=range(len(features))\n",
    "labels.index=range(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_and_fit_model(X_train,y_train,X_test=None,y_test=None,hn=32,dp=0.5,layers=1,epochs=1,batches=64,verbose=0):\n",
    "    input_dim=X_train.shape[1]\n",
    "    output_dim=len(y_train.unique())\n",
    "    Y_train=np_utils.to_categorical(y_train.cat.rename_categories(range(len(y_train.unique()))))\n",
    "    print output_dim, input_dim\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hn, input_shape=(input_dim,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(dp))\n",
    "\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(hn))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dp))\n",
    "\n",
    "    model.add(Dense(output_dim))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    if X_test is not None:\n",
    "        Y_test=np_utils.to_categorical(y_test.cat.rename_categories(range(len(y_test.unique()))))\n",
    "        fitting=model.fit(X_train, Y_train, nb_epoch=epochs, batch_size=batches,verbose=verbose,validation_data=(X_test,Y_test))\n",
    "        test_score = log_loss(y_test, model.predict_proba(X_test,verbose=0))\n",
    "    else:\n",
    "        model.fit(X_train, Y_train, nb_epoch=epochs, batch_size=batches,verbose=verbose)\n",
    "        fitting=0\n",
    "        test_score = 0\n",
    "    return test_score, fitting, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS=20\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "DP=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 70\n",
      "Train on 438991 samples, validate on 438988 samples\n",
      "Epoch 1/20\n",
      "29s - loss: 2.4242 - val_loss: 2.2241\n",
      "Epoch 2/20\n",
      "28s - loss: 2.2882 - val_loss: 2.2121\n",
      "Epoch 3/20\n",
      "27s - loss: 2.2734 - val_loss: 2.2050\n",
      "Epoch 4/20\n",
      "30s - loss: 2.2665 - val_loss: 2.2027\n",
      "Epoch 5/20\n",
      "34s - loss: 2.2618 - val_loss: 2.1974\n",
      "Epoch 6/20\n",
      "34s - loss: 2.2593 - val_loss: 2.1967\n",
      "Epoch 7/20\n",
      "29s - loss: 2.2560 - val_loss: 2.1949\n",
      "Epoch 8/20\n",
      "29s - loss: 2.2536 - val_loss: 2.1934\n",
      "Epoch 9/20\n",
      "28s - loss: 2.2518 - val_loss: 2.1909\n",
      "Epoch 10/20\n",
      "28s - loss: 2.2490 - val_loss: 2.1892\n",
      "Epoch 11/20\n",
      "28s - loss: 2.2493 - val_loss: 2.1894\n",
      "Epoch 12/20\n",
      "29s - loss: 2.2486 - val_loss: 2.1864\n",
      "Epoch 13/20\n",
      "29s - loss: 2.2468 - val_loss: 2.1865\n",
      "Epoch 14/20\n",
      "29s - loss: 2.2454 - val_loss: 2.1877\n",
      "Epoch 15/20\n",
      "29s - loss: 2.2452 - val_loss: 2.1868\n",
      "Epoch 16/20\n",
      "28s - loss: 2.2443 - val_loss: 2.1870\n",
      "Epoch 17/20\n",
      "29s - loss: 2.2442 - val_loss: 2.1852\n",
      "Epoch 18/20\n",
      "29s - loss: 2.2437 - val_loss: 2.1848\n",
      "Epoch 19/20\n",
      "29s - loss: 2.2437 - val_loss: 2.1850\n",
      "Epoch 20/20\n",
      "29s - loss: 2.2435 - val_loss: 2.1842\n"
     ]
    }
   ],
   "source": [
    "score, fitting, model = build_and_fit_model(features_train.as_matrix(),labels_train,X_test=features_test.as_matrix(),y_test=labels_test,hn=N_HN,layers=N_LAYERS,epochs=N_EPOCHS,verbose=2,dp=DP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 2.18099860518\n",
      "train 2.177833624\n",
      "test 2.18416160688\n"
     ]
    }
   ],
   "source": [
    "print \"all\", log_loss(labels, model.predict_proba(features.as_matrix(),verbose=0))\n",
    "print \"train\", log_loss(labels_train, model.predict_proba(features_train.as_matrix(),verbose=0))\n",
    "print \"test\", log_loss(labels_test, model.predict_proba(features_test.as_matrix(),verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13b2c57d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plt.plot(fitting.history['val_loss'],label=\"validation\")\n",
    "plt.plot(fitting.history['loss'],label=\"train\")\n",
    "# plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 70\n",
      "Epoch 1/20\n",
      "53s - loss: 2.3542\n",
      "Epoch 2/20\n",
      "57s - loss: 2.2704\n",
      "Epoch 3/20\n",
      "58s - loss: 2.2610\n",
      "Epoch 4/20\n",
      "52s - loss: 2.2547\n",
      "Epoch 5/20\n",
      "54s - loss: 2.2521\n",
      "Epoch 6/20\n",
      "53s - loss: 2.2491\n",
      "Epoch 7/20\n",
      "51s - loss: 2.2470\n",
      "Epoch 8/20\n",
      "52s - loss: 2.2460\n",
      "Epoch 9/20\n",
      "54s - loss: 2.2454\n",
      "Epoch 10/20\n",
      "55s - loss: 2.2432\n",
      "Epoch 11/20\n",
      "53s - loss: 2.2431\n",
      "Epoch 12/20\n",
      "55s - loss: 2.2419\n",
      "Epoch 13/20\n",
      "60s - loss: 2.2424\n",
      "Epoch 14/20\n",
      "55s - loss: 2.2406\n",
      "Epoch 15/20\n",
      "57s - loss: 2.2406\n",
      "Epoch 16/20\n",
      "50s - loss: 2.2406\n",
      "Epoch 17/20\n",
      "50s - loss: 2.2396\n",
      "Epoch 18/20\n",
      "49s - loss: 2.2394\n",
      "Epoch 19/20\n",
      "49s - loss: 2.2386\n",
      "Epoch 20/20\n",
      "49s - loss: 2.2381\n"
     ]
    }
   ],
   "source": [
    "score, fitting, model = build_and_fit_model(features.as_matrix(),labels,hn=N_HN,layers=N_LAYERS,epochs=N_EPOCHS,verbose=2,dp=DP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 2.1753961041\n",
      "train 2.17610193679\n",
      "test 2.17468864752\n"
     ]
    }
   ],
   "source": [
    "print \"all\", log_loss(labels, model.predict_proba(features.as_matrix(),verbose=0))\n",
    "print \"train\", log_loss(labels_train, model.predict_proba(features_train.as_matrix(),verbose=0))\n",
    "print \"test\", log_loss(labels_test, model.predict_proba(features_test.as_matrix(),verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDF=pd.read_csv(\"../data/test.csv\")\n",
    "testDF[[\"X\",\"Y\"]]=xy_scaler.transform(testDF[[\"X\",\"Y\"]])\n",
    "#set outliers to 0\n",
    "testDF[\"X\"]=testDF[\"X\"].apply(lambda x: 0 if abs(x)>5 else x)\n",
    "testDF[\"Y\"]=testDF[\"Y\"].apply(lambda y: 0 if abs(y)>5 else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_addresses=sorted(testDF[\"Address\"].unique())\n",
    "new_A_counts=testDF.groupby(\"Address\").size()\n",
    "only_new=set(new_addresses+addresses)-set(addresses)\n",
    "only_old=set(new_addresses+addresses)-set(new_addresses)\n",
    "in_both=set(new_addresses).intersection(addresses)\n",
    "for addr in only_new:\n",
    "    PA=new_A_counts[addr]/float(len(testDF)+len(trainDF))\n",
    "    logoddsPA[addr]=np.log(PA)-np.log(1.-PA)\n",
    "    logodds[addr]=deepcopy(default_logodds)\n",
    "    logodds[addr].index=range(len(categories))\n",
    "for addr in in_both:\n",
    "    PA=(A_counts[addr]+new_A_counts[addr])/float(len(testDF)+len(trainDF))\n",
    "    logoddsPA[addr]=np.log(PA)-np.log(1.-PA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating address features\n",
      "Parsing dates\n"
     ]
    }
   ],
   "source": [
    "features_sub, _=parse_data(testDF,logodds,logoddsPA)\n",
    "# scaler.fit(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collist=features_sub.columns.tolist()\n",
    "print collist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_sub[collist]=scaler.transform(features_sub[collist])\n",
    "\n",
    "predDF=pd.DataFrame(model.predict_proba(features_sub.as_matrix(),verbose=0),columns=sorted(labels.unique()))\n",
    "predDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predDF.to_csv(\"Crime_SF_NN_Logodds.csv\",index_label=\"Id\",na_rep=\"0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
